{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "american-hearts",
   "metadata": {},
   "source": [
    "## Selection bias with COMPAS data\n",
    "This notebook demonstrates the effect of selection bias on fairness using COMPAS data. <br>\n",
    "In this notebook, we first import packages needed in this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulated-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"models\")\n",
    "sys.path.append(\"AIF360/\")\n",
    "import numpy as np\n",
    "from compas_model import get_distortion_compas_sel, CompasDataset_test, CompasDataset_train, get_evaluation, load_preproc_data_compas_test\n",
    "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-hands",
   "metadata": {},
   "source": [
    "The function below process data and create a dataset with selection bias. <br>\n",
    "In COMPAS dataset, we have race as sensitive attribute and use age (binned into 3 bins), priors_count (number of prior criminal cases, binned into 3 bins), c_charge_degree (degree charged by the prosecutor) and score_text (a risk score assigned to the defendant) as features to predict two_year_recid (if person will be re-arrested for a violent offense within two years). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "verified-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preproc_data_compas_train(protected_attributes=None):\n",
    "    def custom_preprocessing(df):\n",
    "        \"\"\"The custom pre-processing function is adapted from\n",
    "            https://github.com/fair-preprocessing/nips2017/blob/master/compas/code/Generate_Compas_Data.ipynb\n",
    "        \"\"\"\n",
    "\n",
    "        df = df[['age',\n",
    "                 'c_charge_degree',\n",
    "                 'race',\n",
    "                 'age_cat',\n",
    "                 'score_text',\n",
    "                 'sex',\n",
    "                 'priors_count',\n",
    "                 'days_b_screening_arrest',\n",
    "                 'decile_score',\n",
    "                 'is_recid',\n",
    "                 'two_year_recid',\n",
    "                 'length_of_stay']]\n",
    "\n",
    "        # Indices of data samples to keep\n",
    "        ix = df['days_b_screening_arrest'] <= 30\n",
    "        ix = (df['days_b_screening_arrest'] >= -30) & ix\n",
    "        ix = (df['is_recid'] != -1) & ix\n",
    "        ix = (df['c_charge_degree'] != \"O\") & ix\n",
    "        ix = (df['score_text'] != 'N/A') & ix\n",
    "        df = df.loc[ix, :]\n",
    "\n",
    "        # Restrict races to African-American and Caucasian\n",
    "        dfcut = df.loc[~df['race'].isin(\n",
    "            ['Native American', 'Hispanic', 'Asian', 'Other']), :]\n",
    "\n",
    "        # Restrict the features to use\n",
    "        dfcutQ = dfcut[['sex',\n",
    "                        'race',\n",
    "                        'age_cat',\n",
    "                        'c_charge_degree',\n",
    "                        'score_text',\n",
    "                        'priors_count',\n",
    "                        'is_recid',\n",
    "                        'two_year_recid',\n",
    "                        'length_of_stay']].copy()\n",
    "\n",
    "        # Quantize priors count between 0, 1-3, and >3\n",
    "        def quantizePrior(x):\n",
    "            if x == 0:\n",
    "                return '0'\n",
    "            elif x == 1:\n",
    "                return '1 to 3'\n",
    "            elif x == 2:\n",
    "                return 'More than 3'\n",
    "            else:\n",
    "                return 'missing'\n",
    "        # Quantize length of stay\n",
    "\n",
    "        def quantizeLOS(x):\n",
    "            if x == 0:\n",
    "                return '<week'\n",
    "            if x == 1:\n",
    "                return '<3months'\n",
    "            else:\n",
    "                return '>3 months'\n",
    "\n",
    "        # Quantize length of stay\n",
    "        def adjustAge(x):\n",
    "            if x == 1:\n",
    "                return '25 to 45'\n",
    "            elif x == 2:\n",
    "                return 'Greater than 45'\n",
    "            elif x == 0:\n",
    "                return 'Less than 25'\n",
    "        # Quantize score_text to MediumHigh\n",
    "\n",
    "        def quantizeScore(x):\n",
    "            if x == 1:\n",
    "                return 'MediumHigh'\n",
    "            else:\n",
    "                return 'Low'\n",
    "\n",
    "        def group_race(x):\n",
    "            if x == \"Caucasian\":\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "        dfcutQ['priors_count'] = dfcutQ['priors_count'].apply(\n",
    "            lambda x: quantizePrior(x))\n",
    "        dfcutQ['length_of_stay'] = dfcutQ['length_of_stay'].apply(\n",
    "            lambda x: quantizeLOS(x))\n",
    "        dfcutQ['score_text'] = dfcutQ['score_text'].apply(\n",
    "            lambda x: quantizeScore(x))\n",
    "        dfcutQ['age_cat'] = dfcutQ['age_cat'].apply(lambda x: adjustAge(x))\n",
    "        # Recode sex and race\n",
    "        dfcutQ['sex'] = dfcutQ['sex'].replace({'Female': 1.0, 'Male': 0.0})\n",
    "        dfcutQ['race'] = dfcutQ['race'].apply(lambda x: group_race(x))\n",
    "\n",
    "        features = ['two_year_recid', 'race',\n",
    "                    'age_cat', 'priors_count', 'c_charge_degree', 'score_text']\n",
    "\n",
    "        # Pass vallue to df\n",
    "        df = dfcutQ[features]\n",
    "        # Here, we filter out dataframe with negative outcome\n",
    "        df_neg = df.loc[df['two_year_recid'] == 1, :]\n",
    "        # df_neg_priv represents observations with negative outcome in privileged group\n",
    "        df_neg_priv = df_neg.loc[(df_neg['two_year_recid'] == 1) & (\n",
    "            df_neg['race'] == 1), :]\n",
    "        # df_neg_unpriv represents observations with negative outcome in unprivileged group\n",
    "        df_neg_unpriv = df_neg.loc[(df_neg['two_year_recid'] == 1) & (\n",
    "            df_neg['race'] == 0), :]\n",
    "        # the code below will create a biased dataset for observations with negative outcome. \n",
    "        # We randomly select observations from df_neg_unpriv and df_neg_priv to create\n",
    "        # a new dataset with selection bias \n",
    "        _, df_neg_priv_test = train_test_split(\n",
    "            df_neg_priv, test_size=500, random_state=10)\n",
    "        _, df_neg_unpriv_test = train_test_split(\n",
    "            df_neg_unpriv, test_size=950, random_state=10)\n",
    "        df_neg_test = df_neg_priv_test.append(df_neg_unpriv_test)\n",
    "        print('negative outcome, unpriv before resampling')\n",
    "        print(len(df_neg_unpriv_test.index))\n",
    "\n",
    "        print('negative outcome, priv before resampling')\n",
    "        print(len(df_neg_priv_test.index))\n",
    "        # Here, we filter out dataframe with positive outcome\n",
    "        df_pos = df.loc[df['two_year_recid'] == 0, :]\n",
    "        # df_pos_priv represents observations with positive outcome in privileged group\n",
    "        df_pos_priv = df_pos.loc[(df_pos['two_year_recid'] == 0) & (\n",
    "            df_pos['race'] == 1), :]\n",
    "        # df_pos_unpriv represents observations with positive outcome in unprivileged group\n",
    "        df_pos_unpriv = df_pos.loc[(df_pos['two_year_recid'] == 0) & (\n",
    "            df_pos['race'] == 0), :]\n",
    "        # the code below will create a biased dataset for observations with positive outcome. \n",
    "        # We randomly select observations from df_pos_unpriv and df_pos_priv to create\n",
    "        # a new dataset with selection bias.\n",
    "        _, df_pos_priv_test = train_test_split(\n",
    "            df_pos_priv, test_size=650, random_state=10)\n",
    "        _, df_pos_unpriv_test = train_test_split(\n",
    "            df_pos_unpriv, test_size=900, random_state=10)\n",
    "        df_pos_test = df_pos_priv_test.append(df_pos_unpriv_test)\n",
    "        print('positive outcome, unpriv before resampling')\n",
    "        print(len(df_pos_unpriv_test.index))\n",
    "\n",
    "        print('positive outcome, priv before resampling')\n",
    "        print(len(df_pos_priv_test.index))\n",
    "        df = df_neg_test.append(df_pos_test)\n",
    "\n",
    "        return df\n",
    "\n",
    "    XD_features = [\n",
    "        'age_cat',\n",
    "        'c_charge_degree',\n",
    "        'priors_count',\n",
    "        'race',\n",
    "        'score_text']\n",
    "    D_features = [\n",
    "        'race'] if protected_attributes is None else protected_attributes\n",
    "    Y_features = ['two_year_recid']\n",
    "    X_features = list(set(XD_features) - set(D_features))\n",
    "    categorical_features = [\n",
    "        'age_cat',\n",
    "        'priors_count',\n",
    "        'c_charge_degree',\n",
    "        'score_text']\n",
    "\n",
    "    # privileged classes\n",
    "    all_privileged_classes = {\"sex\": [1.0],\n",
    "                              \"race\": [1.0]}\n",
    "\n",
    "    # protected attribute maps\n",
    "    all_protected_attribute_maps = {\n",
    "        \"sex\": {\n",
    "            0.0: 'Male', 1.0: 'Female'}, \"race\": {\n",
    "            1.0: 'Caucasian', 0.0: 'Not Caucasian'}}\n",
    "\n",
    "    return CompasDataset_train(\n",
    "        label_name=Y_features[0],\n",
    "        favorable_classes=[0],\n",
    "        protected_attribute_names=D_features,\n",
    "        privileged_classes=[all_privileged_classes[x] for x in D_features],\n",
    "        instance_weights_name=None,\n",
    "        categorical_features=categorical_features,\n",
    "        features_to_keep=X_features + Y_features + D_features,\n",
    "        na_values=[],\n",
    "        metadata={'label_maps': [{1.0: 'Did recid.', 0.0: 'No recid.'}],\n",
    "                  'protected_attribute_maps': [all_protected_attribute_maps[x]\n",
    "                                               for x in D_features]},\n",
    "        custom_preprocessing=custom_preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-blink",
   "metadata": {},
   "source": [
    "The code below is to load the data and run the fairness fixing algorithm proposed by Calmon et al. \\[1\\]. We then use the processed data to train a logistic regression classifier and validate the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "clinical-hungarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative outcome, unpriv before resampling\n",
      "950\n",
      "negative outcome, priv before resampling\n",
      "500\n",
      "positive outcome, unpriv before resampling\n",
      "900\n",
      "positive outcome, priv before resampling\n",
      "650\n",
      "Optimized Preprocessing: Objective converged to 0.000000\n",
      "Without resampling\n",
      "Accuracy\n",
      "0.654510556621881\n",
      "p-rule\n",
      "0.8197167755991285\n",
      "FPR for unpriv group\n",
      "0.36690647482014394\n",
      "FNR for unpriv group\n",
      "0.39520958083832336\n",
      "FPR for priv group\n",
      "0.1834862385321101\n",
      "FNR for priv group\n",
      "0.4056603773584906\n"
     ]
    }
   ],
   "source": [
    "privileged_groups = [{'race': 1}]\n",
    "unprivileged_groups = [{'race': 0}]\n",
    "dataset_orig_vt = load_preproc_data_compas_test(['race'])\n",
    "dataset_orig_train = load_preproc_data_compas_train(['race'])\n",
    "\n",
    "optim_options = {\n",
    "    \"distortion_fun\": get_distortion_compas_sel,\n",
    "    \"epsilon\": 0.05,\n",
    "    \"clist\": [0.99, 1.99, 2.99],\n",
    "    \"dlist\": [.1, 0.05, 0]\n",
    "}\n",
    "\n",
    "OP = OptimPreproc(OptTools, optim_options,\n",
    "                  unprivileged_groups=unprivileged_groups,\n",
    "                  privileged_groups=privileged_groups)\n",
    "\n",
    "OP = OP.fit(dataset_orig_train)\n",
    "\n",
    "dataset_transf_cat_test = OP.transform(dataset_orig_vt, transform_Y=True)\n",
    "dataset_transf_cat_test = dataset_orig_vt.align_datasets(\n",
    "    dataset_transf_cat_test)\n",
    "\n",
    "dataset_transf_cat_train = OP.transform(\n",
    "    dataset_orig_train, transform_Y=True)\n",
    "dataset_transf_cat_train = dataset_orig_train.align_datasets(\n",
    "    dataset_transf_cat_train)\n",
    "\n",
    "scale_transf = StandardScaler()\n",
    "X_train = dataset_orig_train.features\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "X_test = scale_transf.fit_transform(dataset_orig_vt.features)\n",
    "\n",
    "scale_transf = StandardScaler()\n",
    "X_train = scale_transf.fit_transform(dataset_transf_cat_train.features)\n",
    "y_train = dataset_transf_cat_train.labels.ravel()\n",
    "\n",
    "X_test = scale_transf.fit_transform(dataset_transf_cat_test.features)\n",
    "\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train)\n",
    "y_pred = lmod.predict(X_test)\n",
    "print('Without resampling')\n",
    "get_evaluation(dataset_orig_vt,y_pred,privileged_groups,unprivileged_groups,0,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-intellectual",
   "metadata": {},
   "source": [
    "The code below does uniform rasampling method on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "individual-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preproc_data_compas_train(protected_attributes=None):\n",
    "    def custom_preprocessing(df):\n",
    "        \"\"\"The custom pre-processing function is adapted from\n",
    "            https://github.com/fair-preprocessing/nips2017/blob/master/compas/code/Generate_Compas_Data.ipynb\n",
    "        \"\"\"\n",
    "\n",
    "        df = df[['age',\n",
    "                 'c_charge_degree',\n",
    "                 'race',\n",
    "                 'age_cat',\n",
    "                 'score_text',\n",
    "                 'sex',\n",
    "                 'priors_count',\n",
    "                 'days_b_screening_arrest',\n",
    "                 'decile_score',\n",
    "                 'is_recid',\n",
    "                 'two_year_recid',\n",
    "                 'length_of_stay']]\n",
    "\n",
    "        # Indices of data samples to keep\n",
    "        ix = df['days_b_screening_arrest'] <= 30\n",
    "        ix = (df['days_b_screening_arrest'] >= -30) & ix\n",
    "        ix = (df['is_recid'] != -1) & ix\n",
    "        ix = (df['c_charge_degree'] != \"O\") & ix\n",
    "        ix = (df['score_text'] != 'N/A') & ix\n",
    "        df = df.loc[ix, :]\n",
    "\n",
    "        # Restrict races to African-American and Caucasian\n",
    "        dfcut = df.loc[~df['race'].isin(\n",
    "            ['Native American', 'Hispanic', 'Asian', 'Other']), :]\n",
    "\n",
    "        # Restrict the features to use\n",
    "        dfcutQ = dfcut[['sex',\n",
    "                        'race',\n",
    "                        'age_cat',\n",
    "                        'c_charge_degree',\n",
    "                        'score_text',\n",
    "                        'priors_count',\n",
    "                        'is_recid',\n",
    "                        'two_year_recid',\n",
    "                        'length_of_stay']].copy()\n",
    "\n",
    "        # Quantize priors count between 0, 1-3, and >3\n",
    "        def quantizePrior(x):\n",
    "            if x == 0:\n",
    "                return '0'\n",
    "            elif x == 1:\n",
    "                return '1 to 3'\n",
    "            elif x == 2:\n",
    "                return 'More than 3'\n",
    "            else:\n",
    "                return 'missing'\n",
    "        # Quantize length of stay\n",
    "\n",
    "        def quantizeLOS(x):\n",
    "            if x == 0:\n",
    "                return '<week'\n",
    "            if x == 1:\n",
    "                return '<3months'\n",
    "            else:\n",
    "                return '>3 months'\n",
    "\n",
    "        # Quantize length of stay\n",
    "        def adjustAge(x):\n",
    "            if x == 1:\n",
    "                return '25 to 45'\n",
    "            elif x == 2:\n",
    "                return 'Greater than 45'\n",
    "            elif x == 0:\n",
    "                return 'Less than 25'\n",
    "        # Quantize score_text to MediumHigh\n",
    "\n",
    "        def quantizeScore(x):\n",
    "            if x == 1:\n",
    "                return 'MediumHigh'\n",
    "            else:\n",
    "                return 'Low'\n",
    "\n",
    "        def group_race(x):\n",
    "            if x == \"Caucasian\":\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "        dfcutQ['priors_count'] = dfcutQ['priors_count'].apply(\n",
    "            lambda x: quantizePrior(x))\n",
    "        dfcutQ['length_of_stay'] = dfcutQ['length_of_stay'].apply(\n",
    "            lambda x: quantizeLOS(x))\n",
    "        dfcutQ['score_text'] = dfcutQ['score_text'].apply(\n",
    "            lambda x: quantizeScore(x))\n",
    "        dfcutQ['age_cat'] = dfcutQ['age_cat'].apply(lambda x: adjustAge(x))\n",
    "        # Recode sex and race\n",
    "        dfcutQ['sex'] = dfcutQ['sex'].replace({'Female': 1.0, 'Male': 0.0})\n",
    "        dfcutQ['race'] = dfcutQ['race'].apply(lambda x: group_race(x))\n",
    "\n",
    "        features = ['two_year_recid', 'race',\n",
    "                    'age_cat', 'priors_count', 'c_charge_degree', 'score_text']\n",
    "\n",
    "        # Pass vallue to df\n",
    "        df = dfcutQ[features]\n",
    "        # This part of the code is the same as the previous function to obtain a \n",
    "        # training data with selection bias\n",
    "        df_neg = df.loc[df['two_year_recid'] == 1, :]\n",
    "        df_neg_priv = df_neg.loc[(df_neg['two_year_recid'] == 1) & (\n",
    "            df_neg['race'] == 1), :]\n",
    "        df_neg_unpriv = df_neg.loc[(df_neg['two_year_recid'] == 1) & (\n",
    "            df_neg['race'] == 0), :]\n",
    "\n",
    "        _, df_neg_priv_test = train_test_split(\n",
    "            df_neg_priv, test_size=500, random_state=10)\n",
    "        _, df_neg_unpriv_test = train_test_split(\n",
    "            df_neg_unpriv, test_size=950, random_state=10)\n",
    "        df_neg_test = df_neg_priv_test.append(df_neg_unpriv_test)\n",
    "        print('negative outcome, unpriv before resampling')\n",
    "        print(len(df_neg_unpriv_test.index))\n",
    "\n",
    "        print('negative outcome, priv before resampling')\n",
    "        print(len(df_neg_priv_test.index))\n",
    "        df_pos = df.loc[df['two_year_recid'] == 0, :]\n",
    "        df_pos_priv = df_pos.loc[(df_pos['two_year_recid'] == 0) & (\n",
    "            df_pos['race'] == 1), :]\n",
    "        df_pos_unpriv = df_pos.loc[(df_pos['two_year_recid'] == 0) & (\n",
    "            df_pos['race'] == 0), :]\n",
    "        _, df_pos_priv_test = train_test_split(\n",
    "            df_pos_priv, test_size=650, random_state=10)\n",
    "        _, df_pos_unpriv_test = train_test_split(\n",
    "            df_pos_unpriv, test_size=900, random_state=10)\n",
    "        df_pos_test = df_pos_priv_test.append(df_pos_unpriv_test)\n",
    "        print('positive outcome, unpriv before resampling')\n",
    "        print(len(df_pos_unpriv_test.index))\n",
    "\n",
    "        print('positive outcome, priv before resampling')\n",
    "        print(len(df_pos_priv_test.index))\n",
    "        df = df_neg_test.append(df_pos_test)\n",
    "        \n",
    "        df_result = pd.DataFrame()\n",
    "        # In this part, we preform uniform resampling described in the paper so that\n",
    "        # the training data has no selection bias\n",
    "        N = len(df)\n",
    "        for i in df['two_year_recid'].unique():\n",
    "            for j in df['race'].unique():\n",
    "                orig_df = df.loc[(df['two_year_recid'] == i)\n",
    "                                 & (df['race'] == j), :]\n",
    "                # real_count is the number of observations in the original data\n",
    "                real_count = len(orig_df.index)\n",
    "                # exp_count is the expected number of obsercations given statistical independence\n",
    "                exp_count = int((len(df.loc[(df['two_year_recid'] == i), :].index) / len(\n",
    "                    df.index)) * (len(df.loc[(df['race'] == j), :].index) / len(df.index)) * N)\n",
    "                # if real_count is bigger than exp_count, we randomly drop some samples \n",
    "                if real_count >= exp_count:\n",
    "                    _, df_toapp = train_test_split(\n",
    "                        orig_df, test_size=exp_count, random_state=10)\n",
    "                # if real_count is smaller than exp_count, we bootstrap from the original data to\n",
    "                # reach statistical independence\n",
    "                else:\n",
    "                    df_toapp = resample(\n",
    "                        orig_df,\n",
    "                        replace=True,\n",
    "                        n_samples=exp_count -\n",
    "                        real_count,\n",
    "                        random_state=10)\n",
    "                    df_toapp = df_toapp.append(orig_df)\n",
    "                if len(df_result.index) == 0:\n",
    "                    df_result = df_toapp.copy()\n",
    "                else:\n",
    "                    df_result = df_result.append(df_toapp)\n",
    "        df = df_result\n",
    "        \n",
    "        return df\n",
    "\n",
    "    XD_features = [\n",
    "        'age_cat',\n",
    "        'c_charge_degree',\n",
    "        'priors_count',\n",
    "        'race',\n",
    "        'score_text']\n",
    "    D_features = [\n",
    "        'race'] if protected_attributes is None else protected_attributes\n",
    "    Y_features = ['two_year_recid']\n",
    "    X_features = list(set(XD_features) - set(D_features))\n",
    "    categorical_features = [\n",
    "        'age_cat',\n",
    "        'priors_count',\n",
    "        'c_charge_degree',\n",
    "        'score_text']\n",
    "\n",
    "    # privileged classes\n",
    "    all_privileged_classes = {\"sex\": [1.0],\n",
    "                              \"race\": [1.0]}\n",
    "\n",
    "    # protected attribute maps\n",
    "    all_protected_attribute_maps = {\n",
    "        \"sex\": {\n",
    "            0.0: 'Male', 1.0: 'Female'}, \"race\": {\n",
    "            1.0: 'Caucasian', 0.0: 'Not Caucasian'}}\n",
    "\n",
    "    return CompasDataset_train(\n",
    "        label_name=Y_features[0],\n",
    "        favorable_classes=[0],\n",
    "        protected_attribute_names=D_features,\n",
    "        privileged_classes=[all_privileged_classes[x] for x in D_features],\n",
    "        instance_weights_name=None,\n",
    "        categorical_features=categorical_features,\n",
    "        features_to_keep=X_features + Y_features + D_features,\n",
    "        na_values=[],\n",
    "        metadata={'label_maps': [{1.0: 'Did recid.', 0.0: 'No recid.'}],\n",
    "                  'protected_attribute_maps': [all_protected_attribute_maps[x]\n",
    "                                               for x in D_features]},\n",
    "        custom_preprocessing=custom_preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-rebate",
   "metadata": {},
   "source": [
    "We load the data after resampling and run the fairness fixing algorithm proposed by Calmon et al. We then use the processed data to train a new logistic regression classifier and validate the classifier on the same test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lined-triple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative outcome, unpriv before resampling\n",
      "950\n",
      "negative outcome, priv before resampling\n",
      "500\n",
      "positive outcome, unpriv before resampling\n",
      "900\n",
      "positive outcome, priv before resampling\n",
      "650\n",
      "Optimized Preprocessing: Objective converged to 0.000000\n",
      "With resampling\n",
      "Accuracy\n",
      "0.6468330134357005\n",
      "p-rule\n",
      "0.917153120790301\n",
      "FPR for unpriv group\n",
      "0.3165467625899281\n",
      "FNR for unpriv group\n",
      "0.45508982035928147\n",
      "FPR for priv group\n",
      "0.19266055045871555\n",
      "FNR for priv group\n",
      "0.4056603773584906\n"
     ]
    }
   ],
   "source": [
    "privileged_groups = [{'race': 1}]\n",
    "unprivileged_groups = [{'race': 0}]\n",
    "dataset_orig_vt = load_preproc_data_compas_test(['race'])\n",
    "dataset_orig_train = load_preproc_data_compas_train(['race'])\n",
    "\n",
    "optim_options = {\n",
    "    \"distortion_fun\": get_distortion_compas_sel,\n",
    "    \"epsilon\": 0.05,\n",
    "    \"clist\": [0.99, 1.99, 2.99],\n",
    "    \"dlist\": [.1, 0.05, 0]\n",
    "}\n",
    "\n",
    "OP = OptimPreproc(OptTools, optim_options,\n",
    "                  unprivileged_groups=unprivileged_groups,\n",
    "                  privileged_groups=privileged_groups)\n",
    "\n",
    "OP = OP.fit(dataset_orig_train)\n",
    "\n",
    "dataset_transf_cat_test = OP.transform(dataset_orig_vt, transform_Y=True)\n",
    "dataset_transf_cat_test = dataset_orig_vt.align_datasets(\n",
    "    dataset_transf_cat_test)\n",
    "\n",
    "dataset_transf_cat_train = OP.transform(\n",
    "    dataset_orig_train, transform_Y=True)\n",
    "dataset_transf_cat_train = dataset_orig_train.align_datasets(\n",
    "    dataset_transf_cat_train)\n",
    "\n",
    "scale_transf = StandardScaler()\n",
    "X_train = dataset_orig_train.features\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "X_test = scale_transf.fit_transform(dataset_orig_vt.features)\n",
    "\n",
    "scale_transf = StandardScaler()\n",
    "X_train = scale_transf.fit_transform(dataset_transf_cat_train.features)\n",
    "y_train = dataset_transf_cat_train.labels.ravel()\n",
    "\n",
    "X_test = scale_transf.fit_transform(dataset_transf_cat_test.features)\n",
    "\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train)\n",
    "y_pred = lmod.predict(X_test)\n",
    "print('With resampling')\n",
    "get_evaluation(dataset_orig_vt,y_pred,privileged_groups,unprivileged_groups,0,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-worst",
   "metadata": {},
   "source": [
    "By comparing the two results, the fairness scores increase with a small tradeoff in accuracy (about 0.8\\% decrease in accuracy) <br>\n",
    "# Reference\n",
    "[1] Optimized Pre-Processing for Discrimination Prevention <br>\n",
    "Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy and Kush R. Varshney.\n",
    "31st Advances in Neural Information Processing Systems (NIPS), Long Beach, CA, December 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-limitation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
